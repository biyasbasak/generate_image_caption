{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
        }
      }
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAxOdskQhtEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ddd22ae-b734-4b2f-c24d-17a35fb88d8a"
      },
      "source": [
        "!pip install nltk\n",
        "!pip3 install tqdm\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbSqacWe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8546fe4c-490d-4df0-9909-365eb8b7893c"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Embedding, LSTM, add\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM_SNyzsxoEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf200d8-1a4a-44d5-fd91-10b68fd929ec"
      },
      "source": [
        "# comment it out if working on local machine. Only required when using google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDIoElWx9-g"
      },
      "source": [
        "CAPTION_FILE_PATH = \"captions.txt\"\n",
        "IMAGE_FOLDER_PATH = \"Images\"\n",
        "IMAGE_PICKLE = \"image_pickle.p\"\n",
        "W2V_PICKLE_PATH = \"w2v_embeddings.p\"\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X06HSXqYe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8224191f-2aa5-4965-d31c-59631ec8c58e"
      },
      "source": [
        "df = pd.read_csv(CAPTION_FILE_PATH)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eb5b027192ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCAPTION_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'captions.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfDC-nYe4sS"
      },
      "source": [
        "images = df.iloc[:, 0]\n",
        "captions = df.iloc[:, 1]\n",
        "print(images.shape)\n",
        "print(captions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia95ZnkAKoxs"
      },
      "source": [
        "print(\"images>>>>\")\n",
        "print(images[0:10])\n",
        "print(\"captions>>>>>\")\n",
        "print(captions[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZdOU3Ude4sT"
      },
      "source": [
        "#### preprocess the captions #######\n",
        "for i, caption in enumerate(captions):\n",
        "    _caption = caption.lower()\n",
        "    tokens = _caption.split()\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    # remove tokens with numbers in them\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    #Add start and end sequence tokens\n",
        "    captions[i] = \"<start> \" + \" \".join(tokens) + \" <end>\"\n",
        "\n",
        "\n",
        "print(captions[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHIzcKxjFp5"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(images, captions, random_state=0)\n",
        "print(train_x[0:5])\n",
        "print(train_y[0:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0Ql7JmQDe4sT"
      },
      "source": [
        "#### Create a vocabulary from the train captions in train_y #####\n",
        "#Eliminate infrequently occurring words from the vocabulary\n",
        "\n",
        "vocabulary = set()\n",
        "word_counts = {}\n",
        "\n",
        "for caption in train_y:\n",
        "    tokens = caption.split(\" \")\n",
        "    for token in tokens:\n",
        "        vocabulary.add(token)\n",
        "        if token in word_counts:\n",
        "            word_counts[token] += 1\n",
        "        else:\n",
        "            word_counts[token] = 1\n",
        "\n",
        "core_vocab = [key for key in word_counts if word_counts[key] >= 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "rKNuD8KFe4sT"
      },
      "source": [
        "#### create a list of captions corresponding to a particular image ######\n",
        "\n",
        "\n",
        "def create_image_to_caption_dict(images,captions):\n",
        "  image_to_caption = defaultdict(list)\n",
        "  for img_, caption in zip(images, captions):\n",
        "    _image = img_.replace(\".jpg\", \"\").strip()\n",
        "    image_to_caption[_image].append(caption)\n",
        "  return image_to_caption\n",
        "\n",
        "train_image_to_caption = create_image_to_caption_dict(train_x, train_y)\n",
        "test_image_to_caption = create_image_to_caption_dict(test_x, test_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQQsxPE7e4sT"
      },
      "source": [
        "model = InceptionV3(weights=\"imagenet\")\n",
        "model = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTA7dIX5nbXh",
        "tags": []
      },
      "source": [
        "# Convert all the images to size 299x299 as expected by the\n",
        "\n",
        "def encode_images(x):\n",
        "  encoded_images = {}\n",
        "  \n",
        "  for _x in tqdm(x):\n",
        "    image_path = path.join(IMAGE_FOLDER_PATH, _x)\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    X = image.img_to_array(img)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    X = preprocess_input(X)\n",
        "    features = model.predict(X)\n",
        "    X = np.reshape(features, features.shape[1])\n",
        "    encoded_images[_x] = X\n",
        "  return encoded_images\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wslljOU125J"
      },
      "source": [
        "encoded_images = {}\n",
        "\n",
        "\n",
        "if os.path.isfile(IMAGE_PICKLE):\n",
        "    with open(IMAGE_PICKLE, 'rb') as p:\n",
        "        encoded_images = pickle.load(p)\n",
        "else: \n",
        "    encoded_images = encode_images(set(images))\n",
        "    pickle.dump(encoded_images, open(\"image_pickle.p\", \"wb\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hHoIP0yFjj0O"
      },
      "source": [
        "#Create python dictionaries to encode indices of unique words in vocabulary\n",
        "w2idx = {core_vocab[i]: i for i in range(len(core_vocab))}\n",
        "idx2w = {i: core_vocab[i] for i in range(len(core_vocab))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2jDLi3rjj0O"
      },
      "source": [
        "#Get max caption length\n",
        "max_caption_length = max(len(i.split(' ')) for i in captions)\n",
        "print(max_caption_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPfOC4I6jj0P"
      },
      "source": [
        "#Create dictionary of word embeddings for full vocabulary\n",
        "# w2v_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.txt\", binary=False)\n",
        "# embedding_dict = {i: w2v_model[i] for i in core_vocab if i in w2v_model}\n",
        "# pickle.dump(embedding_dict, open(\"w2v_embeddings.p\", \"wb\"))\n",
        "\n",
        "embedding_dict = {}\n",
        "with open(W2V_PICKLE_PATH, 'rb') as p:\n",
        "  embedding_dict = pickle.load(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIHqtPgDjj0P"
      },
      "source": [
        "#Create matrix of word embeddings\n",
        "emb_dimensions = 200\n",
        "\n",
        "word_embeddings = np.zeros((len(core_vocab), emb_dimensions))\n",
        "\n",
        "for word, index in w2idx.items():\n",
        "    if word in embedding_dict:\n",
        "        word_embeddings[index] = embedding_dict[word]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qvLUZGpjj0P"
      },
      "source": [
        "#Create merge architecture model\n",
        "image_inputs = Input(shape=(2048,), name=\"ImageLayer\")\n",
        "image_layer1 = Dropout(0.2)(image_inputs)\n",
        "image_layer2 = Dense(256, activation = 'relu')(image_layer1)\n",
        "\n",
        "caption_inputs = Input(shape = (max_caption_length,), name=\"CaptionLayer\")\n",
        "caption_layer1 = Embedding(len(core_vocab), emb_dimensions, mask_zero = True)(caption_inputs)\n",
        "caption_layer2 = Dropout(0.5)(caption_layer1)\n",
        "caption_layer3 = LSTM(256)(caption_layer2)\n",
        "\n",
        "decoder_layer1 = add([image_layer2, caption_layer3])\n",
        "decoder_layer2 = Dense(256, activation = 'relu')(decoder_layer1)\n",
        "output = Dense(len(core_vocab), activation = 'softmax')(decoder_layer2)\n",
        "\n",
        "model2 = Model(inputs = [image_inputs, caption_inputs], outputs = output)\n",
        "model2.layers[2].set_weights([word_embeddings])\n",
        "model2.layers[2].trainable = False\n",
        "\n",
        "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7W8AOjN3H1W"
      },
      "source": [
        "print(model2.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xW9sYWGjj0P"
      },
      "source": [
        "def generate_data(caption_dict, images, w2idx, max_len, batch_size):\n",
        "    image_sequence, input_words, output_words = [], [], []\n",
        "    n = 0\n",
        "    while True:\n",
        "        for img_name, caption_list in caption_dict.items():\n",
        "            n += 1\n",
        "            photo = images[img_name + \".jpg\"]\n",
        "            for c in caption_list:\n",
        "              # encode the words in caption using word to index dictionary  \n",
        "              encoded_caption = [w2idx[w] for w in c.split(\" \") if w in w2idx]\n",
        "\n",
        "              for i in range(1, len(encoded_caption)):\n",
        "                # input sequence should be upto an index, output should be the index\n",
        "                input_sequence, output_sequence = encoded_caption[:i], encoded_caption[i]\n",
        "                # pad the input sequence to the same number of tokens\n",
        "                input_sequence = pad_sequences([input_sequence], maxlen= max_len)[0]\n",
        "\n",
        "                # encode the ouput sequence as categorical class\n",
        "                output_sequence = to_categorical([output_sequence], num_classes=len(core_vocab))[0]\n",
        "\n",
        "                image_sequence.append(photo)\n",
        "                input_words.append(input_sequence)\n",
        "                output_words.append(output_sequence)\n",
        "\n",
        "            if n == batch_size:\n",
        "              yield([np.array(image_sequence), np.array(input_words)], np.array(output_words))\n",
        "              image_sequence, input_words, output_words = [], [], []\n",
        "              n = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "paKizZK1jj0P"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "num_steps = len(train_x) // batch_size\n",
        "# print(num_steps)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    print(\"---------------------\")\n",
        "    print(\"epoch\", i)\n",
        "    data_generator = generate_data(train_image_to_caption, encoded_images, w2idx, max_caption_length, batch_size)\n",
        "    model2.fit(data_generator, epochs = 1, steps_per_epoch = num_steps)\n",
        "    model2.save('model_epoch_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkcaNMJ40Asc"
      },
      "source": [
        "# load model weights\n",
        "model2.load_weights('model_epoch_9.h5',by_name=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urH8BTCnSqDh"
      },
      "source": [
        "def predict(photo):\n",
        "    start = '<start>'\n",
        "    for i in range(max_caption_length):\n",
        "        sequence = [w2idx[w] for w in start.split() if w in w2idx]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_caption_length)\n",
        "        yhat = model2.predict([photo,sequence], verbose=1) \n",
        "        yhat = np.argmax(yhat)\n",
        "        word = idx2w[yhat]\n",
        "        start += ' ' + word\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    final = start.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQxDgYmLSwLD"
      },
      "source": [
        "for i in range(10):\n",
        "  pic = list(test_image_to_caption.keys())[i]\n",
        "  image = encoded_images[pic + \".jpg\"].reshape((1,2048))\n",
        "  x=plt.imread(IMAGE_FOLDER_PATH + \"/\" + pic + \".jpg\")\n",
        "  plt.imshow(x)\n",
        "  plt.show()\n",
        "  print(predict(image))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}