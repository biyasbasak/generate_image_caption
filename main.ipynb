{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
        }
      }
    },
    "colab": {
      "name": "Copy of main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAxOdskQhtEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e653c9c9-dd1c-4574-d717-505691b441e2"
      },
      "source": [
        "!pip install nltk\n",
        "!pip3 install tqdm\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in c:\\python38\\lib\\site-packages (3.5)\n",
            "Requirement already satisfied: click in c:\\python38\\lib\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (0.17.0)\n",
            "Requirement already satisfied: regex in c:\\python38\\lib\\site-packages (from nltk) (2020.10.15)\n",
            "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (from nltk) (4.50.2)\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
            "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (4.50.2)\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
            "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbSqacWe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605076f7-1137-47e1-b883-2396820f09c9"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from os import path\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Embedding, LSTM, add\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgSMs5zC1Rzj",
        "outputId": "b778ee64-17ea-46da-a6ff-a990e642dd0b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorflow_version` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM_SNyzsxoEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e0f85b-36fb-40a5-8390-2e392a6c15c0"
      },
      "source": [
        "# comment it out if working on local machine. Only required when using google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDIoElWx9-g"
      },
      "source": [
        "CAPTION_FILE_PATH = \"captions.txt\"\n",
        "IMAGE_FOLDER_PATH = \"Images\"\n",
        "TRAIN_IMAGE_PICKLE = \"train_image_pickle.p\"\n",
        "TEST_IMAGE_PICKLE = \"test_image_pickle.p\"\n",
        "W2V_PICKLE_PATH = \"w2v_embeddings.p\"\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X06HSXqYe4sS"
      },
      "source": [
        "df = pd.read_csv(CAPTION_FILE_PATH)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfDC-nYe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cceb70-bd68-4a31-d153-72f1274ef637"
      },
      "source": [
        "images = df.iloc[:, 0]\n",
        "captions = df.iloc[:, 1]\n",
        "print(images.shape)\n",
        "print(captions.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40455,)\n(40455,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia95ZnkAKoxs",
        "outputId": "56cc18cb-af63-47c2-9e77-5faa99c46b46"
      },
      "source": [
        "print(\"images>>>>\")\n",
        "print(images[0:10])\n",
        "print(\"captions>>>>>\")\n",
        "print(captions[0:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images>>>>\n",
            "0    1000268201_693b08cb0e.jpg\n",
            "1    1000268201_693b08cb0e.jpg\n",
            "2    1000268201_693b08cb0e.jpg\n",
            "3    1000268201_693b08cb0e.jpg\n",
            "4    1000268201_693b08cb0e.jpg\n",
            "5    1001773457_577c3a7d70.jpg\n",
            "6    1001773457_577c3a7d70.jpg\n",
            "7    1001773457_577c3a7d70.jpg\n",
            "8    1001773457_577c3a7d70.jpg\n",
            "9    1001773457_577c3a7d70.jpg\n",
            "Name: image, dtype: object\n",
            "captions>>>>>\n",
            "0    A child in a pink dress is climbing up a set o...\n",
            "1                A girl going into a wooden building .\n",
            "2     A little girl climbing into a wooden playhouse .\n",
            "3    A little girl climbing the stairs to her playh...\n",
            "4    A little girl in a pink dress going into a woo...\n",
            "5           A black dog and a spotted dog are fighting\n",
            "6    A black dog and a tri-colored dog playing with...\n",
            "7    A black dog and a white dog with brown spots a...\n",
            "8    Two dogs of different breeds looking at each o...\n",
            "9      Two dogs on pavement moving toward each other .\n",
            "Name: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZdOU3Ude4sT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5daa48b-0de1-4230-c369-9e902a834adb"
      },
      "source": [
        "#### preprocess the captions #######\n",
        "for i, caption in enumerate(captions):\n",
        "    _caption = caption.lower()\n",
        "    tokens = _caption.split()\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    # token length greater than one. removes dangling characters\n",
        "    tokens = [word for word in tokens if len(word)>1]\n",
        "    # remove tokens with numbers in them\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    #Add start and end sequence tokens\n",
        "    captions[i] = \"<start> \" + \" \".join(tokens) + \" <end>\"\n",
        "\n",
        "\n",
        "print(captions[0:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    <start> child pink dress climbing set stairs e...\n1             <start> girl going wooden building <end>\n2    <start> little girl climbing wooden playhouse ...\n3    <start> little girl climbing stairs playhouse ...\n4    <start> little girl pink dress going wooden ca...\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHIzcKxjFp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb18482-0778-48e0-c3aa-6f4e6604d06d"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(images, captions)\n",
        "print(train_x[0:5])\n",
        "print(train_y[0:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35183    3726590391_bc6e729bb6.jpg\n22720    3216762979_813c45a8ec.jpg\n31952    3584829998_25e59fdef3.jpg\n32405    3603870481_1ebc696d91.jpg\n16033    2873445888_8764699246.jpg\nName: image, dtype: object\n35183    <start> woman head hand mouth sitting beside s...\n22720                              <start> boy field <end>\n31952            <start> dogs playing baseball field <end>\n32405    <start> black dog holding orange object water ...\n16033          <start> little boy kicking feet water <end>\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0Ql7JmQDe4sT"
      },
      "source": [
        "#### Create a vocabulary from the train captions in train_y #####\n",
        "#Eliminate infrequently occurring words from the vocabulary\n",
        "\n",
        "vocabulary = set()\n",
        "word_counts = {}\n",
        "\n",
        "for caption in train_y:\n",
        "    tokens = caption.split(\" \")\n",
        "    for token in tokens:\n",
        "        vocabulary.add(token)\n",
        "        if token in word_counts:\n",
        "            word_counts[token] += 1\n",
        "        else:\n",
        "            word_counts[token] = 1\n",
        "\n",
        "core_vocab = [key for key in word_counts if word_counts[key] >= 5]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "rKNuD8KFe4sT"
      },
      "source": [
        "#### create a list of captions corresponding to a particular image ######\n",
        "\n",
        "\n",
        "def create_image_to_caption_dict(images,captions):\n",
        "  image_to_caption = defaultdict(list)\n",
        "  for img_, caption in zip(images, captions):\n",
        "    _image = img_.replace(\".jpg\", \"\").strip()\n",
        "    image_to_caption[_image].append(caption)\n",
        "  return image_to_caption\n",
        "\n",
        "train_image_to_caption = create_image_to_caption_dict(train_x, train_y)\n",
        "test_image_to_caption = create_image_to_caption_dict(test_y, test_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQQsxPE7e4sT"
      },
      "source": [
        "model = InceptionV3(weights=\"imagenet\")\n",
        "model = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTA7dIX5nbXh",
        "tags": []
      },
      "source": [
        "# Convert all the images to size 299x299 as expected by the\n",
        "\n",
        "def encode_images(x):\n",
        "  encoded_images = {}\n",
        "  \n",
        "  for _x in tqdm(x):\n",
        "    image_path = path.join(IMAGE_FOLDER_PATH, _x)\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    X = image.img_to_array(img)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    X = preprocess_input(X)\n",
        "    features = model.predict(X)\n",
        "    X = np.reshape(features, features.shape[1])\n",
        "    encoded_images[_x] = X\n",
        "  return encoded_images\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wslljOU125J",
        "outputId": "99ca6785-d709-41ba-8fa8-614163b39160"
      },
      "source": [
        "train_encoded_images = {}\n",
        "test_encoded_images = {}\n",
        "\n",
        "if os.path.isfile(TRAIN_IMAGE_PICKLE):\n",
        "    with open(TRAIN_IMAGE_PICKLE, 'rb') as p:\n",
        "        train_encoded_images = pickle.load(p)\n",
        "else: \n",
        "    train_encoded_images = encode_images(set(train_x))\n",
        "    pickle.dump(train_encoded_images, open(\"train_image_pickle.p\", \"wb\"))\n",
        "\n",
        "if os.path.isfile(TEST_IMAGE_PICKLE):\n",
        "    with open(TEST_IMAGE_PICKLE, 'rb') as p:\n",
        "        test_encoded_images = pickle.load(p)\n",
        "else:\n",
        "    test_encoded_images = encode_images(set(test_x))\n",
        "    pickle.dump(train_encoded_images, open(\"test_image_pickle.p\", \"wb\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8082/8082 [17:22<00:00,  7.75it/s]\n",
            "100%|██████████| 6190/6190 [13:06<00:00,  7.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hHoIP0yFjj0O"
      },
      "source": [
        "#Create python dictionaries to encode indices of unique words in vocabulary\n",
        "w2idx = {core_vocab[i]: i for i in range(len(core_vocab))}\n",
        "idx2w = {i: core_vocab[i] for i in range(len(core_vocab))}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2jDLi3rjj0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4313ecd-2ef4-46ca-93f3-1f5781327e38"
      },
      "source": [
        "#Get max caption length\n",
        "max_caption_length = max(len(i.split(' ')) for i in train_y)\n",
        "print(max_caption_length)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPfOC4I6jj0P"
      },
      "source": [
        "#Create dictionary of word embeddings for full vocabulary\n",
        "# w2v_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.txt\", binary=False)\n",
        "# embedding_dict = {i: w2v_model[i] for i in core_vocab if i in w2v_model}\n",
        "# pickle.dump(embedding_dict, open(\"w2v_embeddings.p\", \"wb\"))\n",
        "\n",
        "embedding_dict = {}\n",
        "with open(W2V_PICKLE_PATH, 'rb') as p:\n",
        "  embedding_dict = pickle.load(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIHqtPgDjj0P"
      },
      "source": [
        "#Create matrix of word embeddings\n",
        "emb_dimensions = 200\n",
        "\n",
        "word_embeddings = np.zeros((len(core_vocab), emb_dimensions))\n",
        "\n",
        "for word, index in w2idx.items():\n",
        "    if word in embedding_dict:\n",
        "        word_embeddings[index] = embedding_dict[word]\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qvLUZGpjj0P"
      },
      "source": [
        "#Create merge architecture model\n",
        "image_inputs = Input(shape=(2048,), name=\"ImageLayer\")\n",
        "image_layer1 = Dropout(0.2)(image_inputs)\n",
        "image_layer2 = Dense(128, activation = 'relu')(image_layer1)\n",
        "\n",
        "caption_inputs = Input(shape = (max_caption_length,), name=\"CaptionLayer\")\n",
        "caption_layer1 = Embedding(len(core_vocab), emb_dimensions, mask_zero = True)(caption_inputs)\n",
        "caption_layer2 = Dropout(0.5)(caption_layer1)\n",
        "caption_layer3 = LSTM(128)(caption_layer2)\n",
        "\n",
        "decoder_layer1 = add([image_layer2, caption_layer3])\n",
        "decoder_layer2 = Dense(128, activation = 'relu')(decoder_layer1)\n",
        "output = Dense(len(core_vocab), activation = 'softmax')(decoder_layer2)\n",
        "\n",
        "model2 = Model(inputs = [image_inputs, caption_inputs], outputs = output)\n",
        "model2.layers[2].set_weights([word_embeddings])\n",
        "model2.layers[2].trainable = False\n",
        "\n",
        "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7W8AOjN3H1W",
        "outputId": "e34ec328-d62a-4e6f-817c-869502989b04"
      },
      "source": [
        "print(model2.summary())\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nCaptionLayer (InputLayer)       [(None, 22)]         0                                            \n__________________________________________________________________________________________________\nImageLayer (InputLayer)         [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 22, 200)      489200      CaptionLayer[0][0]               \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 2048)         0           ImageLayer[0][0]                 \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 22, 200)      0           embedding_1[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          262272      dropout_2[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 128)          168448      dropout_3[0][0]                  \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 128)          0           dense_3[0][0]                    \n                                                                 lstm_1[0][0]                     \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 128)          16512       add_1[0][0]                      \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 2446)         315534      dense_4[0][0]                    \n==================================================================================================\nTotal params: 1,251,966\nTrainable params: 762,766\nNon-trainable params: 489,200\n__________________________________________________________________________________________________\nNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xW9sYWGjj0P"
      },
      "source": [
        "def generate_data(caption_dict, images, w2idx, max_len, batch_size):\n",
        "    image_sequence, input_words, output_words = [], [], []\n",
        "    n = 0\n",
        "    while True:\n",
        "        for img_name, caption_list in caption_dict.items():\n",
        "            n += 1\n",
        "            photo = images[img_name + \".jpg\"]\n",
        "            for c in caption_list:\n",
        "              # encode the words in caption using word to index dictionary  \n",
        "              encoded_caption = [w2idx[w] for w in c.split(\" \") if w in w2idx]\n",
        "\n",
        "              for i in range(1, len(encoded_caption)):\n",
        "                # input sequence should be upto an index, output should be the index\n",
        "                input_sequence, output_sequence = encoded_caption[:i], encoded_caption[i]\n",
        "                # pad the input sequence to the same number of tokens\n",
        "                input_sequence = pad_sequences([input_sequence], maxlen= max_len)[0]\n",
        "\n",
        "                # encode the ouput sequence as categorical class\n",
        "                output_sequence = to_categorical([output_sequence], num_classes=len(core_vocab))[0]\n",
        "\n",
        "                image_sequence.append(photo)\n",
        "                input_words.append(input_sequence)\n",
        "                output_words.append(output_sequence)\n",
        "\n",
        "            if n == batch_size:\n",
        "              yield([np.array(image_sequence), np.array(input_words)], np.array(output_words))\n",
        "              image_sequence, input_words, output_words = [], [], []\n",
        "              n = 0"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "paKizZK1jj0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a64eea-c445-4a8c-b448-003e2a71e70c"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 5\n",
        "num_steps = len(train_x) // batch_size\n",
        "# print(num_steps)\n",
        "for i in range(num_epochs):\n",
        "    print(\"---------------------\")\n",
        "    print(\"epoch\", i)\n",
        "    data_generator = generate_data(train_image_to_caption, train_encoded_images, w2idx, max_caption_length, batch_size)\n",
        "    model2.fit(data_generator, epochs = 1, steps_per_epoch = num_steps)\n",
        "    model2.save('model_epoch_' + str(i) + '.h5')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "epoch 0\n",
            "6068/6068 [==============================] - 308s 51ms/step - loss: 4.1804\n",
            "---------------------\n",
            "epoch 1\n",
            "6068/6068 [==============================] - 322s 53ms/step - loss: 3.5326\n",
            "---------------------\n",
            "epoch 2\n",
            "6068/6068 [==============================] - 326s 54ms/step - loss: 3.3273\n",
            "---------------------\n",
            "epoch 3\n",
            "6068/6068 [==============================] - 611s 101ms/step - loss: 3.2217\n",
            "---------------------\n",
            "epoch 4\n",
            "6068/6068 [==============================] - 414s 68ms/step - loss: 3.1576\n",
            "---------------------\n",
            "epoch 5\n",
            "6068/6068 [==============================] - 367s 60ms/step - loss: 3.1116\n",
            "---------------------\n",
            "epoch 6\n",
            "6068/6068 [==============================] - 384s 63ms/step - loss: 3.0782\n",
            "---------------------\n",
            "epoch 7\n",
            "6068/6068 [==============================] - 412s 68ms/step - loss: 3.0547\n",
            "---------------------\n",
            "epoch 8\n",
            "6068/6068 [==============================] - 359s 59ms/step - loss: 3.0364\n",
            "---------------------\n",
            "epoch 9\n",
            "6068/6068 [==============================] - 357s 59ms/step - loss: 3.0192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P95PrEfjj0Q"
      },
      "source": [
        "def generate_caption(image, max_len):\n",
        "    caption_ = recursive_caption(image, '<start>', max_len)\n",
        "    caption_list = caption_.split(' ')[1:-1]\n",
        "    return ' '.join(caption_list)\n",
        "\n",
        "def recursive_caption(image, current_caption, max_len):\n",
        "    caption_indices = [w2idx[i] for i in current_caption.split() if i in w2idx]\n",
        "    padded_caption = pad_sequences([caption_indices], maxlen = max_len + len(caption_indices))\n",
        "\n",
        "    new_token = idx2w[np.argmax(merge_model.predict([image, padded_caption], verbose=0))]\n",
        "    caption = current_caption + new_token\n",
        "\n",
        "    if new_token == '<end>':\n",
        "        return caption\n",
        "    if depth == 0:\n",
        "        return caption\n",
        "        \n",
        "    recursive_caption(image, caption, max_len-1)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpvK-Bpjj0Q"
      },
      "source": [
        "def model_evaluation(model, caption_list, images, max_len):\n",
        "    ground_truth, predictions = [], []\n",
        "    \n",
        "    for key, captions in caption_list.items():\n",
        "\n",
        "        prediction = generate_caption(images[key], max_len)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        g_truth = [caption.split() for caption in captions]\n",
        "        ground_truth.append(g_truth.split())\n",
        "    \n",
        "    bleu_1 = corpus_bleu(ground_truth, predictions, weights = [1.0, 0, 0, 0])\n",
        "    bleu_2 = corpus_bleu(ground_truth, predictions, weights = [.5, .5, 0, 0])\n",
        "    bleu_3 = corpus_bleu(ground_truth, predictions, weights = [.3, .3, .3, 0])\n",
        "    bleu_4 = corpus_bleu(ground_truth, predictions, weights = [.25, .25, .25, .25])\n",
        "    \n",
        "    return [bleu_1, bleu_2, bleu_3, bleu_4]           "
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}