{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAxOdskQhtEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba24b531-4a38-4c50-9a4f-99ae7d680074"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\wolfe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (3.5)\nRequirement already satisfied: click in c:\\users\\wolfe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in c:\\users\\wolfe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (0.16.0)\nRequirement already satisfied: regex in c:\\users\\wolfe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: tqdm in c:\\users\\wolfe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk) (4.49.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbSqacWe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70622edf-4d16-43f2-981c-405d29a39b0e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pickle\n",
        "from os import path\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Embedding, LSTM, add\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\wolfe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X06HSXqYe4sS"
      },
      "source": [
        "df = pd.read_csv(\"captions.txt\")"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfDC-nYe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1bc105-e988-4f3b-fc8f-ed2c5cf486e1"
      },
      "source": [
        "images = df.iloc[:, 0]\n",
        "captions = df.iloc[:, 1]\n",
        "print(images.shape)\n",
        "print(captions.shape)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40455,)\n(40455,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZdOU3Ude4sT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a82b437-8491-4507-97a9-993b277b59cf"
      },
      "source": [
        "#### preprocess the captions #######\n",
        "for i, caption in enumerate(captions):\n",
        "    _caption = caption.lower()\n",
        "    tokens = _caption.split()\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    # token length greater than one. removes dangling characters\n",
        "    tokens = [word for word in tokens if len(word)>1]\n",
        "    # remove tokens with numbers in them\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    #Add start and end sequence tokens\n",
        "    captions[i] = \"<start> \" + \" \".join(tokens) + \" <end>\"\n",
        "\n",
        "\n",
        "print(captions[0:5])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    <start> child pink dress climbing set stairs e...\n1             <start> girl going wooden building <end>\n2    <start> little girl climbing wooden playhouse ...\n3    <start> little girl climbing stairs playhouse ...\n4    <start> little girl pink dress going wooden ca...\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHIzcKxjFp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f531294a-25ba-4e63-a7a4-2846e5911224"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(images, captions)\n",
        "print(train_x[0:5])\n",
        "print(train_y[0:5])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36492      44129946_9eeb385d77.jpg\n34893    3711664623_ef87105ea7.jpg\n8999      241347441_d3dd9b129f.jpg\n7305     2295447147_458cfea65a.jpg\n11672    2584020755_14e2b3e8fc.jpg\nName: image, dtype: object\n36492    <start> sun setting man woman watch boat go <end>\n34893    <start> two adults two boys posing mountains l...\n8999     <start> football player dressed red looks fiel...\n7305                         <start> brown dog field <end>\n11672    <start> child stands front dancing wedding par...\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0Ql7JmQDe4sT"
      },
      "source": [
        "#### Create a vocabulary from the train captions in train_y #####\n",
        "#Eliminate infrequently occurring words from the vocabulary\n",
        "\n",
        "vocabulary = set()\n",
        "word_counts = {}\n",
        "\n",
        "for caption in train_y:\n",
        "    tokens = caption.split(\" \")\n",
        "    for token in tokens:\n",
        "        vocabulary.add(token)\n",
        "        if token in word_counts:\n",
        "            word_counts[token] += 1\n",
        "        else:\n",
        "            word_counts[token] = 1\n",
        "\n",
        "core_vocab = [i for i in word_counts if word_counts[i] >= 5]\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "rKNuD8KFe4sT"
      },
      "source": [
        "#### create a list of captions corresponding to a particular image ######\n",
        "\n",
        "image_to_caption = defaultdict(list)\n",
        "\n",
        "for img_, caption in zip(images, captions):\n",
        "    _image = img_.replace(\".jpg\", \"\").strip()\n",
        "    image_to_caption[_image].append(caption)\n",
        "\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQQsxPE7e4sT"
      },
      "source": [
        "model = InceptionV3(weights=\"imagenet\")\n",
        "model = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTA7dIX5nbXh",
        "tags": []
      },
      "source": [
        "# Convert all the images to size 299x299 as expected by the\n",
        "# encoded_images = []\n",
        "# for image in images:\n",
        "#   path = path.join(\"images\", image)\n",
        "#   img = image.load_img(path, target_size=(299, 299))\n",
        "#   X = image.img_to_array(img)\n",
        "#   X = np.expand_dims(x, axis=0)\n",
        "#   X = preprocess_input(x)\n",
        "#   X = np.reshape(x, x.shape[1])\n",
        "\n",
        "dir_path = os.path.dirname(os.path.realpath(\"__file__\")) + \"\\\\images\\\\\"\n",
        "encoded_images = []\n",
        "\n",
        "for i in os.listdir(dir_path):\n",
        "    image_path = path.join(dir_path, i)\n",
        "    img = preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    X = preprocessing.image.img_to_array(img)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    X = preprocess_input(X)\n",
        "    image_features = model.predict(X, verbose=0)\n",
        "    encoded_images.append(image_features.reshape(2048,))\n",
        "\n",
        "pickle.dump(encoded_images, open(\"image_pickle.p\", \"wb\"))\n",
        "\n",
        "\"\"\"\n",
        "image_path = path.join(dir_path,\"images\\\\1000268201_693b08cb0e.jpg\")\n",
        "img = preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "X = preprocessing.image.img_to_array(img)\n",
        "print(X.shape)\n",
        "X = np.expand_dims(X, axis=0)\n",
        "print(X.shape)\n",
        "X = preprocess_input(X)\n",
        "print(X.shape)\n",
        "\"\"\""
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-124-8b824c74afc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mimage_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mencoded_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m     87\u001b[0m           method.__name__))\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m           model=self)\n\u001b[0m\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     dataset = dataset.map(\n\u001b[1;32m--> 397\u001b[1;33m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1626\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m           preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4048\u001b[0m           \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4049\u001b[0m           \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4050\u001b[1;33m           **self._flat_structure)\n\u001b[0m\u001b[0;32m   4051\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, sloppy, preserve_cardinality, name)\u001b[0m\n\u001b[0;32m   4678\u001b[0m         \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4679\u001b[0m         \u001b[1;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sloppy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4680\u001b[1;33m         sloppy, \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[0;32m   4681\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4682\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Create python dictionaries to encode indices of unique words in vocabulary\n",
        "w2idx = {core_vocab[i]: i for i in range(len(core_vocab))}\n",
        "idx2w = {i: core_vocab[i] for i in range(len(core_vocab))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ],
      "source": [
        "#Get max caption length\n",
        "max_caption_length = max(len(i.split(' ')) for i in train_y)\n",
        "print(max_caption_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create dictionary of word embeddings for full vocabulary\n",
        "w2v_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.txt\", binary=False)\n",
        "embedding_dict = {i: w2v_model[i] for i in core_vocab if i in w2v_model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create matrix of word embeddings\n",
        "emb_dimensions = 200\n",
        "word_embeddings = np.zeros((len(core_vocab), emb_dimensions))\n",
        "for word, index in w2idx.items():\n",
        "    if word in embedding_dict:\n",
        "        word_embeddings[index] = embedding_dict[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create merge architecture model\n",
        "image_inputs = Input(shape=(2048,))\n",
        "image_layer1 = Dropout(0.5)(image_inputs)\n",
        "image_layer2 = Dense(256, activation = 'relu')(image_layer1)\n",
        "\n",
        "caption_inputs = Input(shape = max_caption_length)\n",
        "caption_layer1 = Embedding(len(core_vocab), emb_dimensions, mask_zero = True)(caption_inputs)\n",
        "caption_layer2 = Dropout(0.5)(caption_layer1)\n",
        "caption_layer3 = LSTM(256)(caption_layer2)\n",
        "\n",
        "decoder_layer1 = add([image_layer2, caption_layer3])\n",
        "decoder_layer2 = Dense(256, activation = 'relu')(decoder_layer1)\n",
        "output = Dense(len(core_vocab), activation = 'softmax')(decoder_layer2)\n",
        "\n",
        "merge_model = Model(inputs = [image_inputs, caption_inputs], outputs = output)\n",
        "merge_model.layers[2].set_weights([word_embeddings])\n",
        "merge_model.layers[2].trainable = False\n",
        "\n",
        "merge_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}