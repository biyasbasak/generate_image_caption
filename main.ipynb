{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
        }
      }
    },
    "colab": {
      "name": "Copy of main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAxOdskQhtEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e653c9c9-dd1c-4574-d717-505691b441e2"
      },
      "source": [
        "!pip install nltk\n",
        "!pip3 install tqdm\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in c:\\python38\\lib\\site-packages (3.5)\n",
            "Requirement already satisfied: click in c:\\python38\\lib\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (0.17.0)\n",
            "Requirement already satisfied: regex in c:\\python38\\lib\\site-packages (from nltk) (2020.10.15)\n",
            "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (from nltk) (4.50.2)\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
            "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n",
            "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (4.50.2)\n",
            "WARNING: You are using pip version 20.2.3; however, version 20.3 is available.\n",
            "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbSqacWe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605076f7-1137-47e1-b883-2396820f09c9"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from os import path\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, Embedding, LSTM, add\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgSMs5zC1Rzj",
        "outputId": "b778ee64-17ea-46da-a6ff-a990e642dd0b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorflow_version` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM_SNyzsxoEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e0f85b-36fb-40a5-8390-2e392a6c15c0"
      },
      "source": [
        "# comment it out if working on local machine. Only required when using google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDIoElWx9-g"
      },
      "source": [
        "CAPTION_FILE_PATH = \"captions.txt\"\n",
        "IMAGE_FOLDER_PATH = \"Images\"\n",
        "TRAIN_IMAGE_PICKLE = \"train_image_pickle.p\"\n",
        "TEST_IMAGE_PICKLE = \"test_image_pickle.p\"\n",
        "W2V_PICKLE_PATH = \"w2v_embeddings.p\"\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X06HSXqYe4sS"
      },
      "source": [
        "df = pd.read_csv(CAPTION_FILE_PATH)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfDC-nYe4sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cceb70-bd68-4a31-d153-72f1274ef637"
      },
      "source": [
        "images = df.iloc[:1000, 0]\n",
        "captions = df.iloc[:1000, 1]\n",
        "print(images.shape)\n",
        "print(captions.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000,)\n(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia95ZnkAKoxs",
        "outputId": "56cc18cb-af63-47c2-9e77-5faa99c46b46"
      },
      "source": [
        "print(\"images>>>>\")\n",
        "print(images[0:10])\n",
        "print(\"captions>>>>>\")\n",
        "print(captions[0:10])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images>>>>\n0    1000268201_693b08cb0e.jpg\n1    1000268201_693b08cb0e.jpg\n2    1000268201_693b08cb0e.jpg\n3    1000268201_693b08cb0e.jpg\n4    1000268201_693b08cb0e.jpg\n5    1001773457_577c3a7d70.jpg\n6    1001773457_577c3a7d70.jpg\n7    1001773457_577c3a7d70.jpg\n8    1001773457_577c3a7d70.jpg\n9    1001773457_577c3a7d70.jpg\nName: image, dtype: object\ncaptions>>>>>\n0    A child in a pink dress is climbing up a set o...\n1                A girl going into a wooden building .\n2     A little girl climbing into a wooden playhouse .\n3    A little girl climbing the stairs to her playh...\n4    A little girl in a pink dress going into a woo...\n5           A black dog and a spotted dog are fighting\n6    A black dog and a tri-colored dog playing with...\n7    A black dog and a white dog with brown spots a...\n8    Two dogs of different breeds looking at each o...\n9      Two dogs on pavement moving toward each other .\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZdOU3Ude4sT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5daa48b-0de1-4230-c369-9e902a834adb"
      },
      "source": [
        "#### preprocess the captions #######\n",
        "for i, caption in enumerate(captions):\n",
        "    _caption = caption.lower()\n",
        "    tokens = _caption.split()\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    # token length greater than one. removes dangling characters\n",
        "    tokens = [word for word in tokens if len(word)>1]\n",
        "    # remove tokens with numbers in them\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    #Add start and end sequence tokens\n",
        "    captions[i] = \"<start> \" + \" \".join(tokens) + \" <end>\"\n",
        "\n",
        "\n",
        "print(captions[0:5])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    <start> child pink dress climbing set stairs e...\n1             <start> girl going wooden building <end>\n2    <start> little girl climbing wooden playhouse ...\n3    <start> little girl climbing stairs playhouse ...\n4    <start> little girl pink dress going wooden ca...\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHIzcKxjFp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb18482-0778-48e0-c3aa-6f4e6604d06d"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(images, captions)\n",
        "print(train_x[0:5])\n",
        "print(train_y[0:5])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "581    1122944218_8eb3607403.jpg\n523     111537222_07e56d5a30.jpg\n792    1176580356_9810d877bf.jpg\n911    1224851143_33bcdd299c.jpg\n901    1220401002_3f44b1f3f7.jpg\nName: image, dtype: object\n581    <start> baby holding small black flag moon sta...\n523    <start> woman purple snakeskin pants climbs ro...\n792         <start> brown black dog jumps red ball <end>\n911           <start> little boy holds spoon mouth <end>\n901        <start> two girls laughing outside yard <end>\nName: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "0Ql7JmQDe4sT"
      },
      "source": [
        "#### Create a vocabulary from the train captions in train_y #####\n",
        "#Eliminate infrequently occurring words from the vocabulary\n",
        "\n",
        "vocabulary = set()\n",
        "word_counts = {}\n",
        "\n",
        "for caption in train_y:\n",
        "    tokens = caption.split(\" \")\n",
        "    for token in tokens:\n",
        "        vocabulary.add(token)\n",
        "        if token in word_counts:\n",
        "            word_counts[token] += 1\n",
        "        else:\n",
        "            word_counts[token] = 1\n",
        "\n",
        "core_vocab = [key for key in word_counts if word_counts[key] >= 5]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "rKNuD8KFe4sT"
      },
      "source": [
        "#### create a list of captions corresponding to a particular image ######\n",
        "\n",
        "\n",
        "def create_image_to_caption_dict(images,captions):\n",
        "  image_to_caption = defaultdict(list)\n",
        "  for img_, caption in zip(images, captions):\n",
        "    _image = img_.replace(\".jpg\", \"\").strip()\n",
        "    image_to_caption[_image].append(caption)\n",
        "  return image_to_caption\n",
        "\n",
        "train_image_to_caption = create_image_to_caption_dict(train_x, train_y)\n",
        "test_image_to_caption = create_image_to_caption_dict(test_y, test_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQQsxPE7e4sT"
      },
      "source": [
        "model = InceptionV3(weights=\"imagenet\")\n",
        "model = Model(model.input, model.layers[-2].output)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTA7dIX5nbXh",
        "tags": []
      },
      "source": [
        "# Convert all the images to size 299x299 as expected by the\n",
        "\n",
        "def encode_images(x):\n",
        "  encoded_images = {}\n",
        "  \n",
        "  for _x in tqdm(x):\n",
        "    image_path = path.join(IMAGE_FOLDER_PATH, _x)\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    X = image.img_to_array(img)\n",
        "    X = np.expand_dims(X, axis=0)\n",
        "    X = preprocess_input(X)\n",
        "    features = model.predict(X)\n",
        "    X = np.reshape(features, features.shape[1])\n",
        "    encoded_images[_x] = X\n",
        "  return encoded_images\n",
        "\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wslljOU125J",
        "outputId": "99ca6785-d709-41ba-8fa8-614163b39160"
      },
      "source": [
        "train_encoded_images = {}\n",
        "test_encoded_images = {}\n",
        "\n",
        "if os.path.isfile(TRAIN_IMAGE_PICKLE):\n",
        "    with open(TRAIN_IMAGE_PICKLE, 'rb') as p:\n",
        "        train_encoded_images = pickle.load(p)\n",
        "else:\n",
        "    train_encoded_images = encode_images(set(train_x))\n",
        "    pickle.dump(train_encoded_images, open(\"train_image_pickle.p\", \"wb\"))\n",
        "\n",
        "if os.path.isfile(TEST_IMAGE_PICKLE):\n",
        "    with open(TEST_IMAGE_PICKLE, 'rb') as p:\n",
        "        test_encoded_images = pickle.load(p)\n",
        "else:\n",
        "    test_encoded_images = encode_images(set(test_x))\n",
        "    pickle.dump(train_encoded_images, open(\"test_image_pickle.p\", \"wb\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hHoIP0yFjj0O"
      },
      "source": [
        "#Create python dictionaries to encode indices of unique words in vocabulary\n",
        "w2idx = {core_vocab[i]: i for i in range(len(core_vocab))}\n",
        "idx2w = {i: core_vocab[i] for i in range(len(core_vocab))}"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2jDLi3rjj0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4313ecd-2ef4-46ca-93f3-1f5781327e38"
      },
      "source": [
        "#Get max caption length\n",
        "max_caption_length = max(len(i.split(' ')) for i in train_y)\n",
        "print(max_caption_length)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPfOC4I6jj0P"
      },
      "source": [
        "#Create dictionary of word embeddings for full vocabulary\n",
        "# w2v_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.txt\", binary=False)\n",
        "# embedding_dict = {i: w2v_model[i] for i in core_vocab if i in w2v_model}\n",
        "# pickle.dump(embedding_dict, open(\"w2v_embeddings.p\", \"wb\"))\n",
        "\n",
        "embedding_dict = {}\n",
        "with open(W2V_PICKLE_PATH, 'rb') as p:\n",
        "  embedding_dict = pickle.load(p)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIHqtPgDjj0P"
      },
      "source": [
        "#Create matrix of word embeddings\n",
        "emb_dimensions = 200\n",
        "\n",
        "word_embeddings = np.zeros((len(core_vocab), emb_dimensions))\n",
        "\n",
        "for word, index in w2idx.items():\n",
        "    if word in embedding_dict:\n",
        "        word_embeddings[index] = embedding_dict[word]\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qvLUZGpjj0P"
      },
      "source": [
        "#Create merge architecture model\n",
        "image_inputs = Input(shape=(2048,), name=\"ImageLayer\")\n",
        "image_layer1 = Dropout(0.2)(image_inputs)\n",
        "image_layer2 = Dense(128, activation = 'relu')(image_layer1)\n",
        "\n",
        "caption_inputs = Input(shape = (max_caption_length,), name=\"CaptionLayer\")\n",
        "caption_layer1 = Embedding(len(core_vocab), emb_dimensions, mask_zero = True)(caption_inputs)\n",
        "caption_layer2 = Dropout(0.5)(caption_layer1)\n",
        "caption_layer3 = LSTM(128)(caption_layer2)\n",
        "\n",
        "decoder_layer1 = add([image_layer2, caption_layer3])\n",
        "decoder_layer2 = Dense(128, activation = 'relu')(decoder_layer1)\n",
        "output = Dense(len(core_vocab), activation = 'softmax')(decoder_layer2)\n",
        "\n",
        "model2 = Model(inputs = [image_inputs, caption_inputs], outputs = output)\n",
        "model2.layers[2].set_weights([word_embeddings])\n",
        "model2.layers[2].trainable = False\n",
        "\n",
        "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7W8AOjN3H1W",
        "outputId": "e34ec328-d62a-4e6f-817c-869502989b04"
      },
      "source": [
        "print(model2.summary())\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"functional_9\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nCaptionLayer (InputLayer)       [(None, 20)]         0                                            \n__________________________________________________________________________________________________\nImageLayer (InputLayer)         [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 20, 200)      36800       CaptionLayer[0][0]               \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 2048)         0           ImageLayer[0][0]                 \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 20, 200)      0           embedding_1[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 128)          262272      dropout_2[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 128)          168448      dropout_3[0][0]                  \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 128)          0           dense_3[0][0]                    \n                                                                 lstm_1[0][0]                     \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 128)          16512       add_1[0][0]                      \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 184)          23736       dense_4[0][0]                    \n==================================================================================================\nTotal params: 507,768\nTrainable params: 470,968\nNon-trainable params: 36,800\n__________________________________________________________________________________________________\nNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xW9sYWGjj0P"
      },
      "source": [
        "def generate_data(caption_dict, images, w2idx, max_len, batch_size):\n",
        "    image_sequence, input_words, output_words = [], [], []\n",
        "    n = 0\n",
        "    while True:\n",
        "        for img_name, caption_list in caption_dict.items():\n",
        "            n += 1\n",
        "            photo = images[img_name + \".jpg\"]\n",
        "            for c in caption_list:\n",
        "              # encode the words in caption using word to index dictionary  \n",
        "              encoded_caption = [w2idx[w] for w in c.split(\" \") if w in w2idx]\n",
        "\n",
        "              for i in range(1, len(encoded_caption)):\n",
        "                # input sequence should be upto an index, output should be the index\n",
        "                input_sequence, output_sequence = encoded_caption[:i], encoded_caption[i]\n",
        "                # pad the input sequence to the same number of tokens\n",
        "                input_sequence = pad_sequences([input_sequence], maxlen= max_len)[0]\n",
        "\n",
        "                # encode the ouput sequence as categorical class\n",
        "                output_sequence = to_categorical([output_sequence], num_classes=len(core_vocab))[0]\n",
        "\n",
        "                image_sequence.append(photo)\n",
        "                input_words.append(input_sequence)\n",
        "                output_words.append(output_sequence)\n",
        "\n",
        "            if n == batch_size:\n",
        "              yield([np.array(image_sequence), np.array(input_words)], np.array(output_words))\n",
        "              image_sequence, input_words, output_words = [], [], []\n",
        "              n = 0"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "paKizZK1jj0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a64eea-c445-4a8c-b448-003e2a71e70c"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 5\n",
        "num_steps = len(train_x) // batch_size\n",
        "# print(num_steps)\n",
        "for i in range(num_epochs):\n",
        "    print(\"---------------------\")\n",
        "    print(\"epoch\", i)\n",
        "    data_generator = generate_data(train_image_to_caption, train_encoded_images, w2idx, max_caption_length, batch_size)\n",
        "    model2.fit(data_generator, epochs = 1, steps_per_epoch = num_steps)\n",
        "    model2.save('model_epoch_' + str(i) + '.h5')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "epoch 0\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 2.2501\n",
            "---------------------\n",
            "epoch 1\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 1.9140\n",
            "---------------------\n",
            "epoch 2\n",
            "150/150 [==============================] - 7s 50ms/step - loss: 1.7398\n",
            "---------------------\n",
            "epoch 3\n",
            "150/150 [==============================] - 8s 55ms/step - loss: 1.5855\n",
            "---------------------\n",
            "epoch 4\n",
            "150/150 [==============================] - 7s 44ms/step - loss: 1.4044\n",
            "---------------------\n",
            "epoch 5\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.2692\n",
            "---------------------\n",
            "epoch 6\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.1457\n",
            "---------------------\n",
            "epoch 7\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 1.0726\n",
            "---------------------\n",
            "epoch 8\n",
            "150/150 [==============================] - 6s 40ms/step - loss: 0.9414\n",
            "---------------------\n",
            "epoch 9\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.8759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P95PrEfjj0Q"
      },
      "source": [
        "def generate_caption(image, max_len):\n",
        "    caption_ = recursive_caption(image, '<start>', max_len)\n",
        "    caption_list = caption_.split(' ')[1:-1]\n",
        "    return ' '.join(caption_list)\n",
        "\n",
        "def recursive_caption(image, current_caption, max_len):\n",
        "    caption_indices = [w2idx[i] for i in current_caption.split() if i in w2idx]\n",
        "    padded_caption = pad_sequences([caption_indices], maxlen = max_len + len(caption_indices))\n",
        "\n",
        "    new_token = idx2w[np.argmax(merge_model.predict([image, padded_caption], verbose=0))]\n",
        "    caption = current_caption + new_token\n",
        "\n",
        "    if new_token == '<end>':\n",
        "        return caption\n",
        "    if depth == 0:\n",
        "        return caption\n",
        "        \n",
        "    recursive_caption(image, caption, max_len-1)\n"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpvK-Bpjj0Q"
      },
      "source": [
        "def model_evaluation(model, caption_list, images, max_len):\n",
        "    ground_truth, predictions = [], []\n",
        "    \n",
        "    for key, captions in caption_list.items():\n",
        "\n",
        "        prediction = generate_caption(images[key], max_len)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        g_truth = [caption.split() for caption in captions]\n",
        "        ground_truth.append(g_truth.split())\n",
        "    \n",
        "    bleu_1 = corpus_bleu(ground_truth, predictions, weights = [1.0, 0, 0, 0])\n",
        "    bleu_2 = corpus_bleu(ground_truth, predictions, weights = [.5, .5, 0, 0])\n",
        "    bleu_3 = corpus_bleu(ground_truth, predictions, weights = [.3, .3, .3, 0])\n",
        "    bleu_4 = corpus_bleu(ground_truth, predictions, weights = [.25, .25, .25, .25])\n",
        "    \n",
        "    return [bleu_1, bleu_2, bleu_3, bleu_4]           "
      ],
      "execution_count": 262,
      "outputs": []
    }
  ]
}